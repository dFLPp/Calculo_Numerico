## 4. Solução de sistemas lineares

Dado um sistema linear (conjunto de equações com variáveis em comum) podemos calcular soluções para ele ao utilizar da representação matricial.

O procedimento mais direto porém arcáico é a **Eliminação Gaussiana** (aka: _Escalonamento_)

Como você deve lembrar de algebra linear, o escalonamento consiste em você "zerar" as colunas de modo a formar um "triangulo" de zeros na matriz. No papel nos fazemos isso livremente, mas no computador precisamos achar um padrão, por isso usamos um pivô. O pivô é o primeiro elemento não-nulo de uma linha. Vamos usar o pivô e uma das operações elementares para escalonar um sistema no código abaixo:

```python
import numpy as np
import copy as cp

np.random.seed(2)
A = np.random.rand(5,5)
b = np.random.rand(5,1)

Acp = cp.copy(A)
bcp = cp.copy(b)

print(f"A:\n{A}\n")
print(f"b:\n{b}\n")

# Començando o escalonamento:
#1. Definir um pivo
#2. Definir o fato multiplicativo ideal para zerar cada elem.
pivo = A[0,0]
m = -A[1,0]/pivo
A[1] = A[1] + m*A[0]
b[1] = b[1] + m*b[0]

m = -A[2,0]/pivo
A[2] = A[2] + m*A[0]
b[2] = b[2] + m*b[0]

m = -A[3,0]/pivo
A[3] = A[3] + m*A[0]
b[3] = b[3] + m*b[0]

m = -A[4,0]/pivo
A[4] = A[4] + m*A[0]
b[4] = b[4] + m*b[0]

#Depois da primeira leva de operações
print(f"A:\n{A}\n")
#O objetivo é criar aquele "triangulo de zeros na base na matriz"
```

Repare que existe um certo padrão ai, um padrão que encaixa perfeitamente em um nested loop.
Antes de mostrar o codigo, perceba que isso não é tudo. Depois que escalonarmos a matriz, ainda será necessário calcular manualmente os valores de x1, x2, x3, x4, x5. Mas uma vez, também existe uma forma mais compacta de fazer isso.
Uma observação que, dada a natureza do escalonamento, primeiro é calculado o x5 depois o x4, x3, x2 e finalmente o x1, pois esse ultimo precisa de todos as outras icognitas.

```python
import numpy as np
import pandas as pd

def escalonamento(matriz_coeficientes, termos_independentes):
    A = np.copy(matriz_coeficientes)
    b = np.copy(termos_independentes)
    tamanho = len(A)

    for j in range(0, tamanho-1):
		# pivoteamento ocorreria aqui
        pivo = A[j,j]
        for k in range(j+1, tamanho):
            m = -A[k,j]/pivo
            A[k] = A[k] + m*A[j]
            b[k] = b[k] + m*b[j]
            A[k,j] = 0 #"garantindo" que o elem. foi zerado.

    return A,b

def incognitas(matriz_A_escalonada, matriz_b_escalonada):
    tamanho = len(matriz_A_escalonada)
    A = np.copy(matriz_A_escalonada)
    b = np.copy(matriz_b_escalonada)

    x = np.zeros(tamanho)
    #calculando manualmente a ultima raiz, já que é uma divisão simples
    x[tamanho-1] = b[tamanho-1]/A[tamanho-1,tamanho-1]

    #calculando todas as outras raizes, que seguem um padrão de somatorio
    for k in range(tamanho-2, -1 ,-1):
        soma = b[k]
        for t in range(k, tamanho):
            soma = soma - (A[k,t]*x[t])
        x[k] = soma/A[k,k]
    return x



np.random.seed(10)
A = np.random.rand(10,10)
b = np.random.rand(10,1)
Ars, brs = escalonamento(A,b)
x = incognitas(Ars, brs)

#repare no triangulo de zeros
print(f"Matriz A escalonada:\n{pd.DataFrame(Ars)}\n\n")

#verificando a resposta
print(f"\n\nA x incognitas:\n{A@x}\n\ntermos independentes\n{b}")
```

### Pivoteamento:

O código anterior é valido, mas falho em alguns casos. Se algum dos pivôs for 0 ou um número muito pequeno, ocorrerá um erro numérico que resultará em um _encerramento castastrófico_.
Para evitar isso usa-se do pivoteamento no escalonamento. Pivoteamento é o processo de mudar a linha ou coluna da matriz de modo a obter um pivô ideal.

- Sem pivoteamento: Escalonamento normal. Não altera o valor do pivô. Sujeito a erro de divisão por 0
- Pivoteamento Parcial: Ocorre troca de linhas da matriz para que o pivô seja o maior possivel.
- Pivoteamento total: Ocorre troca de colunas inteiras para que o pivô seja ideal.

Vamos nos focar no Pivoteamento Parcial. Basicamente, antes de você começar a zerar uma coluna, você tem que verificar se o elemento que será o pivô é o maior número dessa coluna, em módulo. Para isso basta fazer `np.argmax(np.abs(A[i,j]))` para achar o indice do maior valor da coluna, e então trocar a linha do pivô padrão pela desse novo pivô.

```
def pivoteamento(A,b,j):
    novoPivo = np.argmax(np.abs(A[j:, j])) + j
    A[[novoPivo, j]] = A[[j, novoPivo]]
    b[[novoPivo, j]] = b[[j, novoPivo]]
```

Basicamente só precisamos executar essa função dentro da função de escalonamento, dentro do loop dela.

Versão compacta:

```
import numpy as np
import pandas as pd

def gss(Ao,bo):
    s = len(Ao)
    A = np.copy(Ao)
    b = np.copy(bo)

    for i in range(0, s-1):
        #pivoteamento
        index = np.argmax(np.abs(A[i:,i]))+i
        A[[index, i]] = A[[i, index]]
        b[[index, i]] = b[[i, index]]

        pivo = A[i,i];
        for j in range(i+1, s):
            k = -A[j, i]/pivo
            A[j] = A[j] + k*A[i]
            b[j] = b[j] + k*b[i]
            A [j,i] = 0

    return A,b

def res(A, b):
    s = len(A)
    x = np.zeros(s)

    #ultima raiz
    x[s-1] = b[s-1]/A[s-1,s-1]

    #outras
    for i in range(s-2, -1, -1):
        sum = b[i]
        for j in range(i, s):
            sum = sum - (A[i,j]*x[j])
        x[i] = sum/A[i,i]
    return x

np.random.seed(5)
Ao = np.random.rand(5,5)
bo = np.random.rand(5,1)

A, b = gss(Ao, bo)
x = res(A, b)
print(pd.DataFrame(A))
print(pd.DataFrame(b))
print(pd.DataFrame(A@x))
```

#### Fatoração LU

Dada uma matriz quadrada A semi-positiva, você vai decompor ela em uma matriz triangular inferior L, que possue os fatores multiplicativos de cada iteração do processo de escalonamento, e uma matriz que triangular superior U, que é o resultado do processo de escalonamento, de modo que **A = L\*U)**. Além disso, podemos calcular pois **L\*Y = b** e **U\*X = Y**

Você vai realizar o escalonamento normalmente a matriz resultante de vai ser U. Durante o processo você vai salvar todos os fatores multiplicativos, e então vai montar com eles a matriz inferior triangular L. OBS: Todos os elementos de de L são negativos, tipo, os elementos são $$-m_ij)$$
Lembrando que o _fator multiplicativo_ citado é um valor $$m = - A[i+1][i]/A[i][i]$$. Você pega o valor na linha de baixo, na mesma coluna, e divide pela valor negativo do elem. de cima. Dessa forma quando você realizar a operação elementar para escalonar essa segunda linha, esse primeiro termo vai sumir.

#### Fatoração Cholesky

É usada para encontrar a solução de sistemas lineares cuja matriz dos coeficientes não é quadrada. Vamos ter A = L\*L^T. Ao invés de resolver o problema, vamos aproximar a distancia da resposta desse sistema (que pode nao ter solução) para um sistema que tem solução. Esse é o metodo de minimos quadrados de algebra linear. Ou seja, queremos minimar o modulo (A\*X - B)². No final temos a equação: **(A^T)\*A\*X = (A^T)\*b**.

Apartir disso, se A é uma matriz simetrica (A = A^T) e se A é positiva definida (os dets das matrizes quadradas que começam no canto sup. esq. e vão crescendo são todas positivas) então temos com consequencia que **A = (L^T) \* L**

###Métodos iterativos
Aqui nós queremos criar uma função/algoritimo do p(x), onde você obtem o x2 ao fazer p(x1)
É usado quando a matriz dos coeficientes tem muitos 0's ou quando o sistema é muito grande
Como normalmente os sistemas são grandes, nos guardamos as respotas, x1, x2, ..., xn em um vetor (do python)

#### Jacobi:

Dado um sistemas de equação, se você pega um valor de x e isola ele, basicamente você vai ter a figura a seguir:
![alt text](/home/flp/Downloads/its about time/1.Material/P3/CN/pics/jacobi-compacto.png)
Basicamente nos estamos isolando um xi e calculando ele em função de todos os outros x. Podemos colocar isso na forma de matriz:
![alt text](/home/flp/Downloads/its about time/1.Material/P3/CN/pics/jacobi-forma-matricial.png)
Aqui da pra ver mais claramente. Em cada linha, você pegar o coeficiente que está na diagonal principal (a) e utilizar o fator multiplicativo $$-1*1/a$$ depois disso você faz uma operação elementar de modo a deixar a diagonal principal igual à zero.
Repare que uma matriz c é criada, nela você simplismente divide o termo independente pelo coeficiente (a).
Você vai começar com valores iniciais de x e como qualquer outro método iterativo, isso pode convergir ou divergir. Uma das maneiras de verificar se o algoritimo vai convergir é calcular todos os autovalores (de algebra linear) e se todos eles forem, em módulo, menor que 1 (0 ponto alguma coisa) então ele converge.
Os criterios de parada mais comum são número max. de iterações e erro máximo menor que algo.

Ou seja, para resolver um problema pelo método de jacobi, você vai transformar os matrizes iniciais A, b nas matrizes D e c, calcular o proximo valor de x com base na equação matricial e então calcular o erro desse x em especifico com o seu anterior.
Claro que você não precisa montar a matriz certinha, fazer o produto de matrizes e etc, você pode usar a formula da imagem 1 para todos os x em uma única iteração.
Repare que como calculamos o erro para cada iteração, o números de "erros"/precisão vai ser igual ao número de variáveis.

####Gauss-Seidel:
É jeito mais rapido. Ele vai se aproveitar da "Proceduralidade" das linguagens. Quando você ver o código vai perceber que o novo valor de x1 (da iteração) vai ser calculado. Depois vai ocorrer o mesmo com o x2, ..., xn e isso é feito usando os valores iniciais.
O que o gauss-seidel faz é: já que você já calculou o novo x1 antes de calcular o x2, no calculo do x2 ao invés de usar o x1 antigo, vou usar o novo (da iteração)
![alt text](/home/flp/Downloads/its about time/1.Material/P3/CN/pics/gauss-seidel-compacto.png)
![alt text](/home/flp/Downloads/its about time/1.Material/P3/CN/pics/forma-matricial-gauss-seidel.png)

```
import numpy as np

def inp():
    n = int(input())
    A = np.zeros((n,n), float)
    b = np.zeros(n, float)
    err = np.zeros(2, float)
    for i in range(n):
        for j in range(n):
            A[i][j] = float(input())
    for k in range(n):
        b[k] = float(input())
    err[0] = float(input())
    err[1] = float(input())
    lim = 100
    return A, b, n, err, lim

def metamorfosis(A, b, n):
    D = np.zeros((n,n), float)
    c = np.zeros(n, float)
    coef = np.diag(A)

    for i in range(n):
        c[i] = b[i]/coef[i]
        for j in range(n):
            D[i,j] = -A[i,j]/coef[i]
            if i == j: D[i, j] = 0 #mesmo esquema do pivot.

    return D, c

def jacobi(D, c, lim, err, n):
    stp = 0; prec = 1;
    xi = np.zeros(n)
    xo = np.zeros(n)

    while (stp < lim) and (prec > err[0]) and (prec < err[1]):
        for i in range(n):
            xi[i] = c[i]
            for j in range(n):
                xi[i] += D[i][j]*xo[j]

        prec = np.sum((1 - xi)**2)**(1/2)
        xo = np.copy(xi) #pointers >> python
        stp += 1
    return xi, stp, prec

def gauss_seidel(D, c, lim, err, n):
    stp = 0; prec = 1;
    xi = np.zeros(n)
    xo = np.zeros(n)
    Di = np.triu(D)
    Ds = np.tril(D)
    while (stp < lim) and (prec > err[0]) and (prec < err[1]):
        for i in range(n):
            xi[i] = c[i]
            for j in range(n):
                xi[i] += Ds[i][j]*xi[j]
                xi[i] += Di[i][j]*xo[j]

        prec = np.sum((1 - xi)**2)**(1/2)
        xo = np.copy(xi) #pointers >> python
        stp += 1
    return xi, stp, prec

```

#### Convergência e mal condicionamento

Se TODOS os autovalores da matriz D forem, em módulo, maior que 1, então tanto jacobi quanto gauss-seidel vão convergir (chegar na solução correta).
Se essa condição não for comprida, nada pode-se dizer. Ou seja, mesmo que que exista algum autovalor maior que 1, é possivel que o sistema convirja.

Outra maneira de verificar a convergencia é analisar a _Dominância_ da matriz. Uma matriz é dita dominante por linha se o elemento da linha que está na diagonal principal é maior que a soma de todos os elementos da linha.
Analogamente é dominante por coluna se TODOS os elementos da diagonal são maiores que a soma, em modulo, dos elementos de suas respectivas colunas.

Se é dominante ou por linha ou por coluna (ou pelos 2) o sistema vai convergir nos métodos de jacobi e gauss-seidel.
Em alguns casos será necessário trocar as linhas da matriz (ordem das equações) para que exista dominância. Como boa prática, teste a dominância na matriz inicial, se não der certo, permute as linhas da matriz e tente, e se não der certo permute as colunas da matriz.
No caso de permutar as colunas você vai precisar trocar a ordem do vetor incognitas x.

Uma matriz (sistema) é mal condicionado, se pequenas mudanças nos coeficientes geram grandes alterações na solução. Uma maneira de medir o condicionamento de uma matriz é calcular o cond(A) e verificar que, se cond(A) >> 1, então a matriz provavelmente está mal condicionada: cond(A) = ||A|| \* ||Inversa(A)|| Onde Inversa(A) é a matriz inversa de A. E estamos calculando a norma dessas matrizes (não é o determinante)

O calculo da norma de uma matriz pode ser feita de várias maneiras:

1. Você soma o módulo dos elems. de todas as linhas. A soma-linha que for maior é o modulo (Norma inf)
2. Você soma o módulo dos elems. de cada coluna. A soma-coluna que for maior é o módulo (Norma 1)